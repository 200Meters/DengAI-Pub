{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "File containting utilities for DengAI Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment is: AzureML\n"
     ]
    }
   ],
   "source": [
    "#Get the current conda environment Jupyter is running in\n",
    "env=!conda info\n",
    "env=str.lower(env[1])\n",
    "if 'keras' in env:\n",
    "    env='keras'\n",
    "elif 'xgboost' in env:\n",
    "    env='xgboost'\n",
    "elif 'azureml' in env:\n",
    "    env='AzureML'\n",
    "else: \n",
    "    env='other env'\n",
    "\n",
    "#Import needed packages\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "#Import statsmodels for ARIMA and other functions\n",
    "#import statsmodels.api as sm\n",
    "#from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "#Preprocessing libraries\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#Pipeline and gridsearch tasks\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Set default grid behavior for Seaborn\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "#Import Unsupervised learning models from Keras\n",
    "if env=='keras':\n",
    "    import tensorflow as tf\n",
    "    import keras\n",
    "    from keras import models, layers\n",
    "    from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    from keras.wrappers.scikit_learn import KerasClassifier\n",
    "    from keras.optimizers import SGD, Adam, Nadam\n",
    "    from tensorflow.keras.optimizers import RMSprop\n",
    "    from keras.layers import LSTM,Dense,GRU,TimeDistributed, Conv1D, InputLayer\n",
    "    from keras.models import Sequential\n",
    "\n",
    "#Import XGBoost\n",
    "if env=='xgboost' or env=='base':\n",
    "    from xgboost import XGBRegressor\n",
    "\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('Environment is: '+env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Loads'''\n",
    "#Load data into dataframes for analysis\n",
    "def load_input_data():\n",
    "    #df_input=pd.read_csv('inputdata/dengue_features_train.csv')\n",
    "    df_input=pd.read_csv('inputdata/training_all.csv')\n",
    "    return df_input\n",
    "\n",
    "def load_label_data():\n",
    "    df_labels=pd.read_csv('inputdata/dengue_labels_train.csv')\n",
    "    return df_labels\n",
    "\n",
    "def load_holdout_data():\n",
    "    #df_holdout=pd.read_csv('inputdata/dengue_features_test.csv')\n",
    "    df_holdout=pd.read_csv('inputdata/holdout_all.csv')\n",
    "    return df_holdout\n",
    "\n",
    "def load_original_data():\n",
    "    df_input=pd.read_csv('inputdata/dengue_features_train.csv')\n",
    "    df_labels=pd.read_csv('inputdata/dengue_labels_train.csv')\n",
    "    df_holdout=pd.read_csv('inputdata/dengue_features_test.csv')\n",
    "    return df_input, df_labels, df_holdout\n",
    "\n",
    "def load_all_data():\n",
    "    df_input=pd.read_csv('inputdata/training_all.csv')\n",
    "    df_labels=pd.read_csv('inputdata/labels_all.csv')\n",
    "    df_holdout=pd.read_csv('inputdata/holdout_all.csv')\n",
    "    return df_input, df_labels, df_holdout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Interpolate data\n",
    "'''\n",
    "#Interpolate missing data for each time series. This uses a simple linear interpolation for each series. Primarily ndvi measures\n",
    "def interp(df):\n",
    "    df.interpolate(method='linear',limit_direction='forward',inplace=True)\n",
    "    return df\n",
    "\n",
    "'''\n",
    "Create time lagged data horizontally\n",
    "'''\n",
    "#Function creates a new feature for each period in the defined lag. Updates both label and training data\n",
    "def create_lag_features(df,lag,end_col=0):\n",
    "    for i in range(lag):\n",
    "        df_lag=df.iloc[:,:end_col]\n",
    "        df_lag=df_lag.shift(periods=i)\n",
    "        df=df.join(df_lag,rsuffix='_shift_'+str(i))\n",
    "    \n",
    "    df=df.iloc[lag:,:]\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Feature Scaling and Engineering'''\n",
    "\n",
    "#create a one-hot encoding scheme for the week number to eliminate effects of week order\n",
    "def week_encoder(df):\n",
    "    enc=OneHotEncoder()\n",
    "    np_week=np.array(df['weekofyear'])\n",
    "    np_week=np.reshape(np_week,(-1,1))\n",
    "    input_onehot=pd.DataFrame(enc.fit_transform(np_week).toarray())\n",
    "\n",
    "    #Get the feature names\n",
    "    f=enc.get_feature_names().tolist()\n",
    "    for name in range(0,len(f)): f[name]='week_'+f[name]\n",
    "    for i in range(0,len(input_onehot.columns)): input_onehot.rename(columns={i:f[i]},inplace=True)\n",
    "    \n",
    "    #add one hot encoded features back to the df\n",
    "    df=df.join(input_onehot)\n",
    "    \n",
    "    #return the df with weeks one-hot encoded and the list of week names\n",
    "    return df, f\n",
    "\n",
    "\n",
    "#Applies a standard scaler to the training and holdout data and returns a df along with column names\n",
    "def scale_standard(df_input,df_holdout,scale_list):\n",
    "    #Set up the scaler with the right features\n",
    "    scaler=StandardScaler()\n",
    "    df_i_sc=df_input[scale_list]\n",
    "    df_h_sc=df_holdout[scale_list]\n",
    "    \n",
    "    #fit and transform scaler\n",
    "    scaler=scaler.fit(df_i_sc)\n",
    "    np_i_sc=scaler.transform(df_i_sc)\n",
    "    np_h_sc=scaler.transform(df_h_sc)\n",
    "    \n",
    "    #update the original dataframes with the scaled values\n",
    "    for i in range(len(scale_list)):\n",
    "        df_input[scale_list[i]]=np_i_sc[:,i]\n",
    "        df_holdout[scale_list[i]]=np_h_sc[:,i]\n",
    "        \n",
    "    return df_input, df_holdout, scaler\n",
    "\n",
    "#Applies a MinMax scaler to the training and holdout data and returns a df along with column names\n",
    "def scale_MinMax(df_input,df_holdout,scale_list,feature_range=(0,1)):\n",
    "    #Set up the scaler with the right features\n",
    "    scaler=MinMaxScaler(feature_range=feature_range)\n",
    "    df_i_sc=df_input[scale_list]\n",
    "    df_h_sc=df_holdout[scale_list]\n",
    "    \n",
    "    #fit and transform scaler\n",
    "    scaler=scaler.fit(df_i_sc)\n",
    "    np_i_sc=scaler.transform(df_i_sc)\n",
    "    np_h_sc=scaler.transform(df_h_sc)\n",
    "    \n",
    "    #update the original dataframes with the scaled values\n",
    "    for i in range(len(scale_list)):\n",
    "        df_input[scale_list[i]]=np_i_sc[:,i]\n",
    "        df_holdout[scale_list[i]]=np_h_sc[:,i]\n",
    "        \n",
    "    return df_input, df_holdout, scaler\n",
    "\n",
    "#Applies a robust scaler to the training and holdout data and returns a df along with column names\n",
    "def scale_robust(df_input,df_holdout,scale_list):\n",
    "    #Set up the scaler with the right features\n",
    "    scaler=RobustScaler()\n",
    "    df_i_sc=df_input[scale_list]\n",
    "    df_h_sc=df_holdout[scale_list]\n",
    "    \n",
    "    #fit and transform scaler\n",
    "    scaler=scaler.fit(df_i_sc)\n",
    "    np_i_sc=scaler.transform(df_i_sc)\n",
    "    np_h_sc=scaler.transform(df_h_sc)\n",
    "    \n",
    "    #update the original dataframes with the scaled values\n",
    "    for i in range(len(scale_list)):\n",
    "        df_input[scale_list[i]]=np_i_sc[:,i]\n",
    "        df_holdout[scale_list[i]]=np_h_sc[:,i]\n",
    "        \n",
    "    return df_input, df_holdout, scaler\n",
    "\n",
    "#Applies a max absolute scaler to the training and holdout data and returns a df along with column names\n",
    "def max_abs_scaler(df_input,df_holdout,scale_list):\n",
    "    #Set up the scaler with the right features\n",
    "    scaler=MaxAbsScaler()\n",
    "    df_i_sc=df_input[scale_list]\n",
    "    df_h_sc=df_holdout[scale_list]\n",
    "    \n",
    "    #fit and transform scaler\n",
    "    scaler=scaler.fit(df_i_sc)\n",
    "    np_i_sc=scaler.transform(df_i_sc)\n",
    "    np_h_sc=scaler.transform(df_h_sc)\n",
    "    \n",
    "    #update the original dataframes with the scaled values\n",
    "    for i in range(len(scale_list)):\n",
    "        df_input[scale_list[i]]=np_i_sc[:,i]\n",
    "        df_holdout[scale_list[i]]=np_h_sc[:,i]\n",
    "        \n",
    "    return df_input, df_holdout, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Splitting'''\n",
    "#reduce df_labels to just the target value\n",
    "def get_targets(df):\n",
    "    df_labels=pd.DataFrame(df['total_cases'])\n",
    "    return df_labels\n",
    "\n",
    "\n",
    "#Create function that only splits a dataset by city\n",
    "def split_by_city(df):\n",
    "    df_sj=df[df['city']=='sj']\n",
    "    df_iq=df[df['city']=='iq']\n",
    "    \n",
    "    return df_sj, df_iq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Feature List Creation'''\n",
    "#Create lists of features for each city and the lag for that feature\n",
    "sj_features=[\n",
    "    'year',\n",
    "    'yearcount',\n",
    "    'weekofyear',\n",
    "    #'outbreak_severity',\n",
    "    'station_max_temp_c',\n",
    "    'station_min_temp_c',\n",
    "    'cum_rain_prior_24_wks',\n",
    "    'avg_max_temp_prior_22_wks',\n",
    "    #'total_cases'\n",
    "    #'avg_max_temp_prior_24_wks',\n",
    "    #'avg_min_temp_prior_21_wks'\n",
    "]\n",
    "\n",
    "sj_lags={\n",
    "    'year':0,\n",
    "    'yearcount':0,\n",
    "    'weekofyear':0,\n",
    "    #'outbreak_severity':0,\n",
    "    'station_max_temp_c':0,\n",
    "    'station_min_temp_c':0,\n",
    "    'cum_rain_prior_24_wks':46,\n",
    "    'avg_max_temp_prior_22_wks':0,\n",
    "    #'total_cases':3\n",
    "    #'avg_max_temp_prior_24_wks':0,\n",
    "    #'avg_min_temp_prior_21_wks':0\n",
    "}\n",
    "\n",
    "iq_features=[\n",
    "    'year',\n",
    "    'yearcount',\n",
    "    'weekofyear',\n",
    "    #'outbreak_severity',\n",
    "    'reanalysis_min_air_temp_k',\n",
    "    'station_max_temp_c',\n",
    "    'cum_rain_prior_22_wks',\n",
    "    #'total_cases'\n",
    "    #'avg_min_temp_prior_4_wks',\n",
    "    #'avg_specific_humidity_prior_4_wks'\n",
    "]\n",
    "\n",
    "iq_lags={\n",
    "    'year':0,\n",
    "    'yearcount':0,\n",
    "    'weekofyear':0,\n",
    "    'outbreak_severity':0,\n",
    "    'reanalysis_min_air_temp_k':0,\n",
    "    'station_max_temp_c':0,\n",
    "    'cum_rain_prior_22_wks':43,\n",
    "    #'total_cases':3\n",
    "    #'avg_min_temp_prior_4_wks':0,\n",
    "    #'avg_specific_humidity_prior_4_wks':0\n",
    "}\n",
    "\n",
    "def get_feature_list(city,lag_names=True):\n",
    "    if city=='sj':\n",
    "        feature_list=[]\n",
    "        if lag_names==True:\n",
    "            for key, value in sj_lags.items():\n",
    "                for i in range(value): feature_list.append(str(key)+'_shift_'+str(i))\n",
    "        else:\n",
    "            for key, value in sj_lags.items(): feature_list.append(str(key))\n",
    "    elif city=='iq':\n",
    "        feature_list=[]\n",
    "        if lag_names==True:\n",
    "            for key, value in iq_lags.items():\n",
    "                for i in range(value): feature_list.append(str(key)+'_shift_'+str(i))\n",
    "        else:\n",
    "            for key, value in iq_lags.items(): feature_list.append(str(key))\n",
    "                \n",
    "    return feature_list\n",
    "\n",
    "#returns a list of features to scale\n",
    "def get_standard_scale_list():\n",
    "    standard_scale_list=['reanalysis_air_temp_k',\n",
    "                         'reanalysis_avg_temp_k',\n",
    "                         'reanalysis_dew_point_temp_k',\n",
    "                         'reanalysis_max_air_temp_k',\n",
    "                         'reanalysis_min_air_temp_k',\n",
    "                         'reanalysis_specific_humidity_g_per_kg',\n",
    "                         'station_avg_temp_c',\n",
    "                         'station_max_temp_c',\n",
    "                         'station_min_temp_c']\n",
    "    return standard_scale_list \n",
    "\n",
    "def get_minmax_scale_list():\n",
    "    minmax_scale_list=['year',\n",
    "                       'yearcount',\n",
    "                       'weekofyear',\n",
    "                       'total_cases',\n",
    "                       'station_diur_temp_rng_c',\n",
    "                       'reanalysis_tdtr_k',\n",
    "                       'precipitation_amt_mm',\n",
    "                       'reanalysis_precip_amt_kg_per_m2',\n",
    "                       'reanalysis_relative_humidity_percent',\n",
    "                       'reanalysis_sat_precip_amt_mm',\n",
    "                       'station_precip_mm','cum_rain_prior_24_wks','cum_rain_prior_22_wks',\n",
    "                       'avg_max_temp_prior_22_wks',\n",
    "                       'avg_specific_humidity_prior_4_wks',\n",
    "                       'avg_max_temp_prior_24_wks',\n",
    "                       'avg_min_temp_prior_21_wks',\n",
    "                       'avg_min_temp_prior_4_wks',\n",
    "                       'outbreak_severity']\n",
    "    \n",
    "    return minmax_scale_list\n",
    "\n",
    "def get_robust_scale_list():\n",
    "    robust_scale_list=['year',\n",
    "                       'yearcount',\n",
    "                       'weekofyear',\n",
    "                       'outbreak_severity',\n",
    "                       'reanalysis_min_air_temp_k',\n",
    "                       'station_max_temp_c',\n",
    "                       'cum_rain_prior_22_wks',\n",
    "                       'station_min_temp_c',\n",
    "                       'cum_rain_prior_24_wks',\n",
    "                       'avg_max_temp_prior_22_wks'\n",
    "                      ]\n",
    "    return robust_scale_list\n",
    "\n",
    "def get_mas_scale_list():\n",
    "    robust_scale_list=['year',\n",
    "                       'yearcount',\n",
    "                       'weekofyear',\n",
    "                       'outbreak_severity',\n",
    "                       'reanalysis_min_air_temp_k',\n",
    "                       'station_max_temp_c',\n",
    "                       'cum_rain_prior_22_wks',\n",
    "                       'station_min_temp_c',\n",
    "                       'cum_rain_prior_24_wks',\n",
    "                       'avg_max_temp_prior_22_wks'\n",
    "                      ]\n",
    "    return robust_scale_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(city,scale_norm=True,lookback=50,train_split=.8,test_split=.1,valid_split=.1,xy_split=.2,time_series_split=True):\n",
    "\n",
    "    #Load data\n",
    "    df_input, df_labels, df_holdout = load_all_data()\n",
    "\n",
    "    #Interp data\n",
    "    df_input=interp(df_input)\n",
    "    df_holdout=interp(df_holdout)\n",
    "\n",
    "    #Designate city to prep data for\n",
    "    df_input=df_input[df_input['city']==city]\n",
    "    df_holdout=df_holdout[df_holdout['city']==city]\n",
    "    \n",
    "    #Create consolidated training file\n",
    "    df_all=df_input.append(df_holdout,ignore_index=True)\n",
    "    df_labels=df_labels[df_labels['city']==city]\n",
    "    df_holdout['total_cases']=0\n",
    "    df_holdout_labels=df_holdout['total_cases'].copy()\n",
    "\n",
    "    #Remember the length of each df to break them back apart\n",
    "    len_input=len(df_input)\n",
    "    len_labels=len(df_labels)\n",
    "    len_holdout=len(df_holdout)\n",
    "\n",
    "    #Scale and normalize\n",
    "    standard_scale_list=get_standard_scale_list()\n",
    "    minmax_scale_list=get_minmax_scale_list()\n",
    "    robust_scale_list=get_robust_scale_list()\n",
    "    mas_scalelist=get_mas_scale_list()\n",
    "\n",
    "    #Scale the data\n",
    "    df_all,df_holdout,scaler_robust=scale_robust(df_all,df_holdout,robust_scale_list)\n",
    "\n",
    "    #Get the lists of features to train\n",
    "    training_feature_list=[]\n",
    "    city_feature_list=get_feature_list(city,lag_names=False)\n",
    "    for i in range(len(city_feature_list)):training_feature_list.append(city_feature_list[i])\n",
    "    df_all_lag=df_all[training_feature_list]\n",
    "\n",
    "    #Create lagged data\n",
    "    df_all_lag=create_lag_features(df_all_lag,lag=lookback,end_col=df_all_lag.shape[1])\n",
    "    df_labels=df_labels.iloc[lookback:,:]\n",
    "    df_labels=df_labels.reset_index(drop=True)\n",
    "    \n",
    "    len_input=len_input-lookback\n",
    "    len_labels=len_labels-lookback\n",
    "    \n",
    "    #Split training data into train and test sets for each city\n",
    "    if time_series_split==True: #Split the test and train set along a time series \n",
    "        df_x_train_city=df_all_lag.iloc[:int(len_input*train_split),:]\n",
    "        df_y_train_city=df_labels.iloc[:int(len_labels*train_split),:]\n",
    "        df_y_train_city.reset_index(inplace=True,drop=True)\n",
    "        df_x_test_city=df_all_lag.iloc[int(len_input*train_split):int(len_input*(train_split+test_split)),:]\n",
    "        df_y_test_city=df_labels.iloc[int(len_labels*train_split):int(len_input*(train_split+test_split)),:]\n",
    "        df_x_valid_city=df_all_lag.iloc[int(len_input*(train_split+test_split)):int(len_input*(train_split+test_split+valid_split)),:]\n",
    "        df_y_valid_city=df_labels.iloc[int(len_labels*(train_split+test_split)):,:]\n",
    "        df_x_holdout_city=df_all_lag.iloc[len_input:]\n",
    "        df_y_holdout_city=df_holdout_labels\n",
    "    else: #split test and train randomly\n",
    "        df_x_valid_city=df_all_lag.iloc[len_input-int(len_input*(valid_split)):len_input,:]\n",
    "        df_y_valid_city=df_labels.iloc[-(int(len_labels*(valid_split))):,:]\n",
    "        df_x_holdout_city=df_all_lag.iloc[len_input:]\n",
    "        df_y_holdout_city=df_holdout_labels\n",
    "        df_x_train_city, df_x_test_city, df_y_train_city, df_y_test_city=train_test_split(\n",
    "            df_all_lag.iloc[:int(len_input*(train_split+test_split)),:],\n",
    "            df_labels.iloc[:int(len_input*(train_split+test_split)),:],\n",
    "            test_size=xy_split, random_state=43)\n",
    "    \n",
    "    return df_x_train_city, df_y_train_city, df_x_test_city, df_y_test_city, df_x_valid_city, df_y_valid_city, df_x_holdout_city, df_y_holdout_city\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data_smote(city,lookback=50,train_split=.8,test_split=.2):\n",
    "    #Load data\n",
    "    df_input, df_labels, df_holdout = load_all_data()\n",
    "\n",
    "    #Interp data\n",
    "    df_input=interp(df_input)\n",
    "    df_holdout=interp(df_holdout)\n",
    "\n",
    "    #Designate city to prep data for\n",
    "    df_input=df_input[df_input['city']==city]\n",
    "    df_holdout=df_holdout[df_holdout['city']==city]\n",
    "    \n",
    "    #Create consolidated training file for creating lookback features in the holdout file\n",
    "    df_all=df_input.append(df_holdout,ignore_index=True)\n",
    "    df_labels=df_labels[df_labels['city']==city]\n",
    "    df_holdout['total_cases']=0\n",
    "    df_holdout_labels=df_holdout['total_cases'].copy()\n",
    "    \n",
    "    #drop city and week start date for SMOTE\n",
    "    df_all.drop(columns=['city','week_start_date'],inplace=True)\n",
    "    \n",
    "    #Remember the length of each df to break them back apart\n",
    "    len_input=len(df_input)\n",
    "    len_labels=len(df_labels)\n",
    "    len_holdout=len(df_holdout)\n",
    "\n",
    "    #Get the lists of features to train\n",
    "    training_feature_list=[]\n",
    "    city_feature_list=get_feature_list(city,lag_names=False)\n",
    "    for i in range(len(city_feature_list)):training_feature_list.append(city_feature_list[i])\n",
    "    df_all_lag=df_all[training_feature_list]\n",
    "\n",
    "    #Create lagged data\n",
    "    df_all_lag=create_lag_features(df_all_lag,lag=lookback,end_col=df_all_lag.shape[1])\n",
    "    df_labels=df_labels.iloc[lookback:,:]\n",
    "    df_labels=df_labels.reset_index(drop=True)\n",
    "    \n",
    "    len_input=len_input-lookback\n",
    "    len_labels=len_labels-lookback\n",
    "    \n",
    "    #split the holdout file away from the input/training file\n",
    "    df_input=df_all.iloc[:len_input,:]\n",
    "    df_holdout=df_all.loc[len_input:,:]\n",
    "    \n",
    "    df_input.to_csv('temp.csv')\n",
    "    \n",
    "    #create smote records for df_input based on outbreak severity\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    smt=SMOTE()\n",
    "    df_sev=df_input['outbreak_severity'] #for purposes of smote, the class variable outbreak severity is the y value\n",
    "    df_input, df_sev = smt.fit_sample(df_input, df_sev) #this should give us a balanced set of variables\n",
    "    df_labels=df_input['total_cases'] #get total cases so the labels align with the input records\n",
    "    \n",
    "    #Scale and normalize\n",
    "    standard_scale_list=get_standard_scale_list()\n",
    "    minmax_scale_list=get_minmax_scale_list()\n",
    "    df_input,df_holdout,scaler_standard=scale_standard(df_input,df_holdout,standard_scale_list)\n",
    "    df_input,df_holdout,scaler_minmax=scale_MinMax(df_input,df_holdout,minmax_scale_list,feature_range=(0,1))\n",
    "    \n",
    "    #randomly create train and test as opposed to time series split\n",
    "    x_train,x_test,y_train,y_test=train_test_split(df_input,df_labels,test_size=test_split,random_state=42)\n",
    "    \n",
    "    #return the datasets\n",
    "    return x_train,y_train,x_test,y_test,df_holdout\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Submission File Creation\n",
    "'''\n",
    "def create_submit_file(y_pred_sj,y_pred_iq):\n",
    "    \n",
    "    #create a single array from the two results\n",
    "    np_submit=np.append(y_pred_sj,y_pred_iq)\n",
    "    \n",
    "    #Round the results to the nearest integer value\n",
    "    np_submit=np_submit.astype(int)\n",
    "    \n",
    "    #Replace any negative values with zero\n",
    "    np_submit=np.where(np_submit>0,np_submit,0)\n",
    "    \n",
    "    #Open the submission file and create a df\n",
    "    df_submit=pd.read_csv('inputdata/submission_format.csv')\n",
    "    \n",
    "    #update the target values (total_cases) with the predictions\n",
    "    df_submit['total_cases']=np_submit\n",
    "    \n",
    "    #write the submission file to a csv\n",
    "    df_submit.to_csv('outputdata/submit_file.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "'''\n",
    "Evaluate results\n",
    "'''\n",
    "def evaluate_results(model_sj,model_iq,df_x_test_sj,df_x_test_iq,df_y_test_sj,df_y_test_sq,target):\n",
    "    y_pred_sj=model_sj.predict(df_x_test_sj)\n",
    "    y_pred_iq=model_iq.predict(df_x_test_iq)\n",
    "    y_pred_combined=np.append(y_pred_sj,y_pred_iq)\n",
    "    y_pred_combined=np.where(y_pred_combined>0,y_pred_combined,0) #remove negative predictions\n",
    "    y_target_combined=np.append(df_y_test_sj[target],df_y_test_iq[target])\n",
    "    print('MAE of SJ: '+ str(mean_absolute_error(df_y_test_sj[target],y_pred_sj)))\n",
    "    print('MAE of IQ: '+ str(mean_absolute_error(df_y_test_iq[target],y_pred_iq)))\n",
    "    print('MAE of Combined: ' + str(mean_absolute_error(y_target_combined,y_pred_combined)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#One time procedure to create a combined file with engineered features for cumulative min/max temperature and humidity\\n#load and interpolate data\\ndf_i,df_l,df_h = load_original_data()\\ndf_i=interp(df_i)\\ndf_h=interp(df_h)\\n\\ndf_i['total_cases']=df_l['total_cases']\\ndf_h['total_cases']=0\\n\\n#Begin SJ\\n#Break out source file and holdout file by city\\ndf_sj=df_i[df_i['city']=='sj']\\ndf_sj_h=df_h[df_h['city']=='sj']\\nlen_sj=len(df_sj)\\nlen_sj_h=len(df_sj_h)\\n\\n#concat training and holdout data\\ndf_sj=df_sj.append(df_sj_h)\\n\\n#Create an outbreak label for SJ\\ndf_sj['outbreak_severity']=0\\ndf_sj.loc[df_sj['total_cases']<=50,'outbreak_severity']=0\\ndf_sj.loc[df_sj['total_cases']>50,'outbreak_severity']=1\\ndf_sj.loc[df_sj['total_cases']>100,'outbreak_severity']=2\\ndf_sj.loc[df_sj['total_cases']>175,'outbreak_severity']=3\\ndf_sj.loc[df_sj['total_cases']>300,'outbreak_severity']=4\\n\\n#Get cumulative totals at various intervals - past 4 to past 25 weeks\\nfor i in range(2,25):\\n    df_sj['cum_rain_prior_'+str(i)+'_wks']=df_sj['precipitation_amt_mm'].rolling(i).sum()\\n\\nfor i in range(2,25):\\n    df_sj['avg_min_temp_prior_'+str(i)+'_wks']=df_sj['station_min_temp_c'].rolling(i).mean()\\n    \\nfor i in range(2,25):\\n    df_sj['avg_max_temp_prior_'+str(i)+'_wks']=df_sj['station_max_temp_c'].rolling(i).mean()\\n    \\nfor i in range(2,25):\\n    df_sj['avg_specific_humidity_prior_'+str(i)+'_wks']=df_sj['reanalysis_specific_humidity_g_per_kg'].rolling(i).mean()\\n    \\nfor i in range(2,25):\\n    df_sj['avg_relative_humidity_prior_'+str(i)+'_wks']=df_sj['reanalysis_relative_humidity_percent'].rolling(i).mean()\\n    \\nfor i in range(2,3):\\n    df_sj['avg_total_cases_'+str(i)+'_wks']=df_sj['total_cases'].rolling(i).mean()\\n    \\nfor i in range(2,3):\\n    df_sj['cum_total_cases_'+str(i)+'_wks']=df_sj['total_cases'].rolling(i).sum()\\n\\n\\n#split the files back apart\\ndf_sj_h=df_sj.iloc[len_sj:]\\ndf_sj=df_sj.iloc[:len_sj]\\n\\n#Begin IQ\\n#Break out source file and holdout file by city\\ndf_iq=df_i[df_i['city']=='iq']\\ndf_iq_h=df_h[df_h['city']=='iq']\\nlen_iq=len(df_iq)\\nlen_iq_h=len(df_iq_h)\\n\\n#concat training and holdout data\\ndf_iq=df_iq.append(df_iq_h)\\n\\n#Create outbreak label for iq\\ndf_iq['outbreak_severity']=0\\ndf_iq.loc[df_iq['total_cases']<=20,'outbreak_severity']=0\\ndf_iq.loc[df_iq['total_cases']>20,'outbreak_severity']=1\\ndf_iq.loc[df_iq['total_cases']>39,'outbreak_severity']=2\\n\\n#Get cumulative totals at various intervals - past 4 to past 25 weeks\\n#Get cumulative rainfall totals at various accumulations - past 4 to past 25 weeks\\nfor i in range(2,25):\\n    df_iq['cum_rain_prior_'+str(i)+'_wks']=df_iq['precipitation_amt_mm'].rolling(i).sum()\\n\\nfor i in range(2,25):\\n    df_iq['avg_min_temp_prior_'+str(i)+'_wks']=df_iq['station_min_temp_c'].rolling(i).mean()\\n    \\nfor i in range(2,25):\\n    df_iq['avg_max_temp_prior_'+str(i)+'_wks']=df_iq['station_max_temp_c'].rolling(i).mean()\\n    \\nfor i in range(2,25):\\n    df_iq['avg_specific_humidity_prior_'+str(i)+'_wks']=df_iq['reanalysis_specific_humidity_g_per_kg'].rolling(i).mean()\\n    \\nfor i in range(2,25):\\n    df_iq['avg_relative_humidity_prior_'+str(i)+'_wks']=df_iq['reanalysis_relative_humidity_percent'].rolling(i).mean()\\n    \\nfor i in range(2,3):\\n    df_iq['avg_total_cases_'+str(i)+'_wks']=df_iq['total_cases'].rolling(i).mean()\\n    \\nfor i in range(2,3):\\n    df_iq['cum_total_cases_'+str(i)+'_wks']=df_iq['total_cases'].rolling(i).sum()\\n\\n#split the files back apart\\ndf_iq_h=df_iq.iloc[len_iq:]\\ndf_iq=df_iq.iloc[:len_iq]\\n\\n#Create train and holdout files\\n#Create the input and holdout files with engineered features\\ndf_all=df_sj.append(df_iq)\\ndf_all.to_csv('training_all_all.csv',index=False)\\ndf_holdout=df_sj_h.append(df_iq_h)\\ndf_holdout.to_csv('holdout_all_all.csv',index=False)\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#One time procedure to create a combined file with engineered features for cumulative min/max temperature and humidity\n",
    "#load and interpolate data\n",
    "df_i,df_l,df_h = load_original_data()\n",
    "df_i=interp(df_i)\n",
    "df_h=interp(df_h)\n",
    "\n",
    "df_i['total_cases']=df_l['total_cases']\n",
    "df_h['total_cases']=0\n",
    "\n",
    "#Begin SJ\n",
    "#Break out source file and holdout file by city\n",
    "df_sj=df_i[df_i['city']=='sj']\n",
    "df_sj_h=df_h[df_h['city']=='sj']\n",
    "len_sj=len(df_sj)\n",
    "len_sj_h=len(df_sj_h)\n",
    "\n",
    "#concat training and holdout data\n",
    "df_sj=df_sj.append(df_sj_h)\n",
    "\n",
    "#Create an outbreak label for SJ\n",
    "df_sj['outbreak_severity']=0\n",
    "df_sj.loc[df_sj['total_cases']<=50,'outbreak_severity']=0\n",
    "df_sj.loc[df_sj['total_cases']>50,'outbreak_severity']=1\n",
    "df_sj.loc[df_sj['total_cases']>100,'outbreak_severity']=2\n",
    "df_sj.loc[df_sj['total_cases']>175,'outbreak_severity']=3\n",
    "df_sj.loc[df_sj['total_cases']>300,'outbreak_severity']=4\n",
    "\n",
    "#Get cumulative totals at various intervals - past 4 to past 25 weeks\n",
    "for i in range(2,25):\n",
    "    df_sj['cum_rain_prior_'+str(i)+'_wks']=df_sj['precipitation_amt_mm'].rolling(i).sum()\n",
    "\n",
    "for i in range(2,25):\n",
    "    df_sj['avg_min_temp_prior_'+str(i)+'_wks']=df_sj['station_min_temp_c'].rolling(i).mean()\n",
    "    \n",
    "for i in range(2,25):\n",
    "    df_sj['avg_max_temp_prior_'+str(i)+'_wks']=df_sj['station_max_temp_c'].rolling(i).mean()\n",
    "    \n",
    "for i in range(2,25):\n",
    "    df_sj['avg_specific_humidity_prior_'+str(i)+'_wks']=df_sj['reanalysis_specific_humidity_g_per_kg'].rolling(i).mean()\n",
    "    \n",
    "for i in range(2,25):\n",
    "    df_sj['avg_relative_humidity_prior_'+str(i)+'_wks']=df_sj['reanalysis_relative_humidity_percent'].rolling(i).mean()\n",
    "    \n",
    "for i in range(2,3):\n",
    "    df_sj['avg_total_cases_'+str(i)+'_wks']=df_sj['total_cases'].rolling(i).mean()\n",
    "    \n",
    "for i in range(2,3):\n",
    "    df_sj['cum_total_cases_'+str(i)+'_wks']=df_sj['total_cases'].rolling(i).sum()\n",
    "\n",
    "\n",
    "#split the files back apart\n",
    "df_sj_h=df_sj.iloc[len_sj:]\n",
    "df_sj=df_sj.iloc[:len_sj]\n",
    "\n",
    "#Begin IQ\n",
    "#Break out source file and holdout file by city\n",
    "df_iq=df_i[df_i['city']=='iq']\n",
    "df_iq_h=df_h[df_h['city']=='iq']\n",
    "len_iq=len(df_iq)\n",
    "len_iq_h=len(df_iq_h)\n",
    "\n",
    "#concat training and holdout data\n",
    "df_iq=df_iq.append(df_iq_h)\n",
    "\n",
    "#Create outbreak label for iq\n",
    "df_iq['outbreak_severity']=0\n",
    "df_iq.loc[df_iq['total_cases']<=20,'outbreak_severity']=0\n",
    "df_iq.loc[df_iq['total_cases']>20,'outbreak_severity']=1\n",
    "df_iq.loc[df_iq['total_cases']>39,'outbreak_severity']=2\n",
    "\n",
    "#Get cumulative totals at various intervals - past 4 to past 25 weeks\n",
    "#Get cumulative rainfall totals at various accumulations - past 4 to past 25 weeks\n",
    "for i in range(2,25):\n",
    "    df_iq['cum_rain_prior_'+str(i)+'_wks']=df_iq['precipitation_amt_mm'].rolling(i).sum()\n",
    "\n",
    "for i in range(2,25):\n",
    "    df_iq['avg_min_temp_prior_'+str(i)+'_wks']=df_iq['station_min_temp_c'].rolling(i).mean()\n",
    "    \n",
    "for i in range(2,25):\n",
    "    df_iq['avg_max_temp_prior_'+str(i)+'_wks']=df_iq['station_max_temp_c'].rolling(i).mean()\n",
    "    \n",
    "for i in range(2,25):\n",
    "    df_iq['avg_specific_humidity_prior_'+str(i)+'_wks']=df_iq['reanalysis_specific_humidity_g_per_kg'].rolling(i).mean()\n",
    "    \n",
    "for i in range(2,25):\n",
    "    df_iq['avg_relative_humidity_prior_'+str(i)+'_wks']=df_iq['reanalysis_relative_humidity_percent'].rolling(i).mean()\n",
    "    \n",
    "for i in range(2,3):\n",
    "    df_iq['avg_total_cases_'+str(i)+'_wks']=df_iq['total_cases'].rolling(i).mean()\n",
    "    \n",
    "for i in range(2,3):\n",
    "    df_iq['cum_total_cases_'+str(i)+'_wks']=df_iq['total_cases'].rolling(i).sum()\n",
    "\n",
    "#split the files back apart\n",
    "df_iq_h=df_iq.iloc[len_iq:]\n",
    "df_iq=df_iq.iloc[:len_iq]\n",
    "\n",
    "#Create train and holdout files\n",
    "#Create the input and holdout files with engineered features\n",
    "df_all=df_sj.append(df_iq)\n",
    "df_all.to_csv('training_all_all.csv',index=False)\n",
    "df_holdout=df_sj_h.append(df_iq_h)\n",
    "df_holdout.to_csv('holdout_all_all.csv',index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
